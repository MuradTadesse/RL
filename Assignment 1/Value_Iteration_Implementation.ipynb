{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64288679-8e13-4ead-b391-76a16c3f000b",
   "metadata": {},
   "source": [
    "# Assignment 1: \n",
    "\n",
    "## Question 1: Value Iteration Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b430a51-5583-439c-9525-915d6c013872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy Grid (Value Iteration):\n",
      "['D' 'D' 'D' 'D' 'D']\n",
      "['D' 'D' 'D' 'D' 'D']\n",
      "['D' 'D' 'D' 'D' 'D']\n",
      "['D' 'D' 'D' 'D' 'D']\n",
      "['R' 'R' 'R' 'R' 'G']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----- Environment and Parameters Setup -----\n",
    "grid_size = 5                   # Define grid size: 5x5 grid\n",
    "goal = (4, 4)                   # Specify goal state (bottom-right corner)\n",
    "gamma = 0.9                     # Discount factor for future rewards\n",
    "reward_goal = 10                # Reward for reaching the goal state\n",
    "reward_step = -1                # Penalty for making any non-goal step\n",
    "\n",
    "# Create a value function table (matrix) initialized to zeros\n",
    "V = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# Define the possible actions and their effects on grid coordinates.\n",
    "# 'U' = Up, 'D' = Down, 'L' = Left, 'R' = Right.\n",
    "actions = {\n",
    "    'U': (-1, 0),  # Move up: decrease row index by 1\n",
    "    'D': (1, 0),   # Move down: increase row index by 1\n",
    "    'L': (0, -1),  # Move left: decrease column index by 1\n",
    "    'R': (0, 1)    # Move right: increase column index by 1\n",
    "}\n",
    "\n",
    "def is_valid_state(x, y):\n",
    "    \"\"\"Check if (x, y) is a valid state within the grid boundaries.\"\"\"\n",
    "    return 0 <= x < grid_size and 0 <= y < grid_size\n",
    "\n",
    "# Set a small threshold epsilon for determining convergence\n",
    "epsilon = 1e-3\n",
    "iteration = 0  # To keep track of the number of iterations\n",
    "\n",
    "# ----- Value Iteration Process -----\n",
    "while True:\n",
    "    delta = 0  # Initialize the maximum change in value for this iteration\n",
    "    new_V = np.copy(V)  # Copy the current value function to update it\n",
    "    \n",
    "    # Iterate over each state in the grid world\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            # Skip updating the value for the goal state since it is terminal.\n",
    "            if (i, j) == goal:\n",
    "                continue\n",
    "            \n",
    "            values = []  # List to store computed values for each possible action\n",
    "            \n",
    "            # Evaluate all possible actions from state (i, j)\n",
    "            for a in actions:\n",
    "                dx, dy = actions[a]             # Get the effect of the action 'a'\n",
    "                new_i, new_j = i + dx, j + dy     # Calculate new state coordinates\n",
    "                \n",
    "                # If the new state is off the grid, remain in the same state.\n",
    "                if not is_valid_state(new_i, new_j):\n",
    "                    new_i, new_j = i, j\n",
    "                    \n",
    "                # Assign immediate reward:\n",
    "                # +10 if the new state is the goal, otherwise -1.\n",
    "                r = reward_goal if (new_i, new_j) == goal else reward_step\n",
    "                \n",
    "                # Compute the value: immediate reward plus discounted value of the resulting state\n",
    "                value = r + gamma * V[new_i, new_j]\n",
    "                values.append(value)\n",
    "            \n",
    "            best_value = max(values)         # Determine the best achievable value from state (i, j)\n",
    "            new_V[i, j] = best_value           # Update the value for state (i, j)\n",
    "            \n",
    "            # Update delta with the maximum change observed for convergence checking.\n",
    "            delta = max(delta, abs(new_V[i, j] - V[i, j]))\n",
    "    \n",
    "    V = new_V  # Update the value function matrix for the next iteration\n",
    "    iteration += 1\n",
    "    \n",
    "    # When the maximum change is less than epsilon, we've converged.\n",
    "    if delta < epsilon:\n",
    "        break\n",
    "\n",
    "# ----- Policy Extraction -----\n",
    "# Create a policy grid with the same dimensions to store the best action for each state.\n",
    "policy = np.full((grid_size, grid_size), '', dtype=object)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        # For the goal state, mark it explicitly.\n",
    "        if (i, j) == goal:\n",
    "            policy[i, j] = 'G'\n",
    "        else:\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            # Evaluate every action to determine the best one.\n",
    "            for a in actions:\n",
    "                dx, dy = actions[a]\n",
    "                new_i, new_j = i + dx, j + dy\n",
    "                if not is_valid_state(new_i, new_j):\n",
    "                    new_i, new_j = i, j\n",
    "                r = reward_goal if (new_i, new_j) == goal else reward_step\n",
    "                value = r + gamma * V[new_i, new_j]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = a\n",
    "            policy[i, j] = best_action\n",
    "\n",
    "# ----- Print the Optimal Policy -----\n",
    "print(\"Optimal Policy Grid (Value Iteration):\")\n",
    "for row in policy:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910f7d7-88df-4181-80c8-57df04f63a18",
   "metadata": {},
   "source": [
    "## Question 2: Q-learning Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa603324-0d3a-452c-af46-b961316f5235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned Q-values (Q-learning):\n",
      "State (0,0):\n",
      "  Action U: -1.39\n",
      "  Action D: -0.43\n",
      "  Action L: -1.39\n",
      "  Action R: -0.43\n",
      "State (0,1):\n",
      "  Action U: -1.05\n",
      "  Action D: 0.63\n",
      "  Action L: -1.58\n",
      "  Action R: -0.32\n",
      "State (0,2):\n",
      "  Action U: -2.26\n",
      "  Action D: 1.81\n",
      "  Action L: -0.69\n",
      "  Action R: -2.16\n",
      "State (0,3):\n",
      "  Action U: -1.85\n",
      "  Action D: 3.12\n",
      "  Action L: -1.91\n",
      "  Action R: -1.54\n",
      "State (0,4):\n",
      "  Action U: 1.93\n",
      "  Action D: 4.58\n",
      "  Action L: -1.59\n",
      "  Action R: -1.43\n",
      "\n",
      "State (1,0):\n",
      "  Action U: -1.39\n",
      "  Action D: 0.63\n",
      "  Action L: -0.43\n",
      "  Action R: 0.63\n",
      "State (1,1):\n",
      "  Action U: -0.43\n",
      "  Action D: 1.81\n",
      "  Action L: -0.43\n",
      "  Action R: 1.81\n",
      "State (1,2):\n",
      "  Action U: 0.63\n",
      "  Action D: 3.12\n",
      "  Action L: 0.63\n",
      "  Action R: 3.12\n",
      "State (1,3):\n",
      "  Action U: 1.81\n",
      "  Action D: 1.81\n",
      "  Action L: 1.81\n",
      "  Action R: 4.58\n",
      "State (1,4):\n",
      "  Action U: 3.12\n",
      "  Action D: 6.20\n",
      "  Action L: 3.12\n",
      "  Action R: 4.58\n",
      "\n",
      "State (2,0):\n",
      "  Action U: -0.44\n",
      "  Action D: -2.42\n",
      "  Action L: -2.54\n",
      "  Action R: 1.81\n",
      "State (2,1):\n",
      "  Action U: 0.31\n",
      "  Action D: 3.12\n",
      "  Action L: -1.96\n",
      "  Action R: 0.58\n",
      "State (2,2):\n",
      "  Action U: -0.10\n",
      "  Action D: 4.58\n",
      "  Action L: -1.53\n",
      "  Action R: -1.60\n",
      "State (2,3):\n",
      "  Action U: 3.12\n",
      "  Action D: 2.55\n",
      "  Action L: 2.02\n",
      "  Action R: 5.75\n",
      "State (2,4):\n",
      "  Action U: 4.58\n",
      "  Action D: 8.00\n",
      "  Action L: 3.08\n",
      "  Action R: 6.20\n",
      "\n",
      "State (3,0):\n",
      "  Action U: -2.05\n",
      "  Action D: -2.04\n",
      "  Action L: -1.85\n",
      "  Action R: 2.48\n",
      "State (3,1):\n",
      "  Action U: 0.89\n",
      "  Action D: -1.43\n",
      "  Action L: -0.38\n",
      "  Action R: 4.58\n",
      "State (3,2):\n",
      "  Action U: 0.82\n",
      "  Action D: 5.58\n",
      "  Action L: 2.04\n",
      "  Action R: 6.20\n",
      "State (3,3):\n",
      "  Action U: 0.37\n",
      "  Action D: 8.00\n",
      "  Action L: 3.25\n",
      "  Action R: 7.47\n",
      "State (3,4):\n",
      "  Action U: 6.20\n",
      "  Action D: 10.00\n",
      "  Action L: 6.20\n",
      "  Action R: 8.00\n",
      "\n",
      "State (4,0):\n",
      "  Action U: -2.02\n",
      "  Action D: -1.85\n",
      "  Action L: -2.07\n",
      "  Action R: -1.97\n",
      "State (4,1):\n",
      "  Action U: -1.43\n",
      "  Action D: -1.43\n",
      "  Action L: -1.84\n",
      "  Action R: -1.50\n",
      "State (4,2):\n",
      "  Action U: -1.31\n",
      "  Action D: 2.36\n",
      "  Action L: -1.08\n",
      "  Action R: 7.96\n",
      "State (4,3):\n",
      "  Action U: 5.99\n",
      "  Action D: 3.75\n",
      "  Action L: 4.06\n",
      "  Action R: 10.00\n",
      "State (4,4):\n",
      "  Action U: 0.00\n",
      "  Action D: 0.00\n",
      "  Action L: 0.00\n",
      "  Action R: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ----- Environment and Parameters Setup -----\n",
    "grid_size = 5                       # Define grid size: 5x5 grid world\n",
    "goal = (4, 4)                       # Specify goal state\n",
    "reward_goal = 10                    # Reward for reaching the goal state\n",
    "reward_step = -1                    # Penalty for taking any non-goal step\n",
    "\n",
    "# Define the set of possible actions in an ordered list.\n",
    "actions_list = ['U', 'D', 'L', 'R']  # List of actions: up, down, left, right\n",
    "# Mapping from actions to the effect on grid coordinates.\n",
    "action_effects = {\n",
    "    'U': (-1, 0),   # Up: move one row up\n",
    "    'D': (1, 0),    # Down: move one row down\n",
    "    'L': (0, -1),   # Left: move one column left\n",
    "    'R': (0, 1)     # Right: move one column right\n",
    "}\n",
    "\n",
    "def is_valid_state(x, y):\n",
    "    \"\"\"Return True if the state (x, y) lies within the grid boundaries.\"\"\"\n",
    "    return 0 <= x < grid_size and 0 <= y < grid_size\n",
    "\n",
    "# ----- Q-learning Parameters -----\n",
    "alpha = 0.5             # Learning rate determines how new experiences affect Q-values\n",
    "gamma = 0.9             # Discount factor for future rewards\n",
    "epsilon = 0.1           # Exploration rate for ε-greedy policy\n",
    "episodes = 1000         # Number of episodes for training the agent\n",
    "max_steps = 100         # Maximum steps per episode to avoid infinite loops\n",
    "\n",
    "# Initialize the Q-table with zeros.\n",
    "# The table size is (grid_size x grid_size x number of actions).\n",
    "Q = np.zeros((grid_size, grid_size, len(actions_list)))\n",
    "\n",
    "# ----- Q-learning Algorithm -----\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)  # Start each episode from the defined starting state (0, 0)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        i, j = state  # Unpack current state's coordinates\n",
    "        \n",
    "        # ε-greedy policy: decide whether to explore or exploit.\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Exploration: choose a random action from the available actions.\n",
    "            action_index = random.randint(0, len(actions_list) - 1)\n",
    "        else:\n",
    "            # Exploitation: choose the best action based on the current Q-value estimates.\n",
    "            action_index = np.argmax(Q[i, j])\n",
    "        action = actions_list[action_index]  # Get the chosen action symbol\n",
    "        \n",
    "        # Determine the new state after taking the chosen action.\n",
    "        dx, dy = action_effects[action]\n",
    "        new_i, new_j = i + dx, j + dy\n",
    "        # If the new state is off the grid, the agent remains in the current state.\n",
    "        if not is_valid_state(new_i, new_j):\n",
    "            new_i, new_j = i, j\n",
    "        \n",
    "        next_state = (new_i, new_j)\n",
    "        \n",
    "        # Assign immediate reward: +10 if the new state is the goal; otherwise -1.\n",
    "        reward = reward_goal if next_state == goal else reward_step\n",
    "        \n",
    "        # ----- Q-learning Update -----\n",
    "        # Fetch current Q-value for state-action pair Q(s,a)\n",
    "        current_q = Q[i, j, action_index]\n",
    "        # Estimate maximum future Q-value for the next state: max_a' Q(s', a')\n",
    "        max_q_next = np.max(Q[new_i, new_j])\n",
    "        # Update the Q-value using the Q-learning update rule.\n",
    "        Q[i, j, action_index] = current_q + alpha * (reward + gamma * max_q_next - current_q)\n",
    "        \n",
    "        # Transition to the next state.\n",
    "        state = next_state\n",
    "        \n",
    "        # If the agent reaches the goal, terminate the episode early.\n",
    "        if state == goal:\n",
    "            break\n",
    "\n",
    "# ----- Print the Learned Q-values for Each State -----\n",
    "print(\"\\nLearned Q-values (Q-learning):\")\n",
    "# Loop over every state in the grid world\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        print(f\"State ({i},{j}):\")\n",
    "        # Display the Q-value for each action in the current state.\n",
    "        for index, action in enumerate(actions_list):\n",
    "            print(f\"  Action {action}: {Q[i, j, index]:.2f}\")\n",
    "    print()  # Add an empty line for better readability between grid rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
